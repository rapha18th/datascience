{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of web_scraper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rapha18th/datascience/blob/master/Copy_of_web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "S-8qWe_JkJj6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[<img src=\"https://github.com/decoderkurt/research_project_school_of_ai_2019/blob/master/oneshotgo/data/res/logo.png?raw=1\" width=70%/>](https://www.theschool.ai/courses/data-lit/)\n",
        "\n",
        "This colab is from Siraj Raval's video \"Build a Web Scraper (LIVE) 2016.11.23\" \n",
        "\n",
        "https://www.youtube.com/watch?v=A0Ac_dKNmH0\n",
        "\n",
        "\n",
        "This code is rewritten for reading assignment of Data Lit Week1. \n",
        "\n",
        "Original code https://github.com/llSourcell/web_scraper_live_demo\n",
        "\n",
        "School of AI Data Lit  https://www.theschool.ai/courses/data-lit\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jpQsM2VQhbT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Install\n",
        "**Install stop-words for cleaning text**"
      ]
    },
    {
      "metadata": {
        "id": "PHL1P61-TEsb",
        "colab_type": "code",
        "outputId": "3f0043ec-f07b-488f-a41f-85f302d1a733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JssED2dShir4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Libraries\n",
        "Lets write a Simple script to get the 20 words and their frequency percentage with highest frequency in an English Wikipedia article. \n",
        "\n",
        "Applications are recommender systems, chatbots and NLP, sentiment analysis, data visualization, market research\n",
        "Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
        "\n",
        "Requests is one of the most downloaded Python packages of all time, pulling in over 7,000,000 downloads every month. HTTP library for pulling pushing and authenticating lets you do Regular expression operations\n",
        "special text string for describing a search pattern. find and replace\n",
        "\n",
        "The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python.\n",
        "comparison, addition, greater than less then parses json, formats it\n",
        "\n",
        "The module provides just one function, tabulate, which takes a list of lists or another tabular data type as the first argument, and outputs a nicely formatted plain-text table:\n",
        "system calls, dealw with user arguments list of common stop words various languages like the\n"
      ]
    },
    {
      "metadata": {
        "id": "YUw3P2iBeD_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Impot libraries**"
      ]
    },
    {
      "metadata": {
        "id": "x4Smaip-SP5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import operator\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "import sys\n",
        "from stop_words import get_stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGyUkrV4hzSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Functions for Scraping\n",
        "**Functions for word list and frequency table**"
      ]
    },
    {
      "metadata": {
        "id": "AIrqv2tbVD5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#get the words\n",
        "def getWordList(url):\n",
        "    word_list = []\n",
        "    \n",
        "    #raw data\n",
        "    source_code = requests.get(url)\n",
        "    \n",
        "    #convert to text\n",
        "    plain_text = source_code.text\n",
        "    \n",
        "    #lxml format\n",
        "    soup = BeautifulSoup(plain_text,'lxml')\n",
        "\n",
        "    #find the words in paragraph tag\n",
        "    for text in soup.findAll('p'):\n",
        "        if text.text is None:\n",
        "            continue\n",
        "        content = text.text\n",
        "        #lowercase and split into an array\n",
        "        words = content.lower().split()\n",
        "\n",
        "        for word in words:\n",
        "            #remove non-chars\n",
        "            cleaned_word = clean_word(word)\n",
        "            #if there is still something there\n",
        "            if len(cleaned_word) > 0:\n",
        "                #add it to our word list\n",
        "                word_list.append(cleaned_word)\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def createFrquencyTable(word_list):\n",
        "    word_count = {}\n",
        "    for word in word_list:\n",
        "        if word in word_count:\n",
        "            word_count[word] += 1\n",
        "        else:\n",
        "            word_count[word] = 1\n",
        "\n",
        "    return word_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6f8Y-YhRdgaU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Functions for cleaning text\n",
        "**Cleaning text data using regex**"
      ]
    },
    {
      "metadata": {
        "id": "HdclAE-7VXok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#clean word with regex\n",
        "def clean_word(word):\n",
        "    cleaned_word = re.sub('[^A-Za-z]+', '', word)\n",
        "    return cleaned_word\n",
        "\n",
        "#remove stopwords\n",
        "def remove_stop_words(frequency_list):\n",
        "    stop_words = get_stop_words('en')\n",
        "\n",
        "    temp_list = []\n",
        "    for key,value in frequency_list:\n",
        "        if key not in stop_words:\n",
        "            temp_list.append([key, value])\n",
        "\n",
        "    return temp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HK5Q_NCSg1LI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Modify these variables\n",
        "1.   **string_query : You can modify this valiable whatever you want to search for**\n",
        "2.   **search_mode : True or False to remove stop words or not**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NRq6XMlHfKG-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keyword you want to search\n",
        "string_query = \"Actuarial Economics\"\n",
        "\n",
        "#to remove stop words or not\n",
        "search_mode = True\n",
        "#search_mode = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_GrLh4lhTPS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Run scraping\n"
      ]
    },
    {
      "metadata": {
        "id": "18EPKNMtVp6-",
        "colab_type": "code",
        "outputId": "9f50ea56-e874-429b-fd03-b9d0ab571f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "#access wiki API. json format. query it for data. search tyep. shows list of possibilities\n",
        "wikipedia_api_link = \"https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch=\"\n",
        "wikipedia_link = \"https://en.wikipedia.org/wiki/\"\n",
        "url = wikipedia_api_link + string_query\n",
        "\n",
        "\n",
        "#try-except block. simple way to deal with exceptions \n",
        "#great for HTTP requests\n",
        "try:\n",
        "    #use requests to retrieve raw data from wiki API URL we\n",
        "    #just constructed\n",
        "    response = requests.get(url)\n",
        "\n",
        "    #format that data as a JSON dictionary\n",
        "    data = json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "    #page title, first option\n",
        "    #show this in web browser\n",
        "    wikipedia_page_tag = data['query']['search'][0]['title']\n",
        "\n",
        "    #get actual wiki page based on retrieved title\n",
        "    url = wikipedia_link + wikipedia_page_tag\n",
        "    #get list of words from that page\n",
        "    page_word_list = getWordList(url)\n",
        "    #create table of word counts, dictionary\n",
        "    page_word_count = createFrquencyTable(page_word_list)\n",
        "    #sort the table by the frequency count\n",
        "    sorted_word_frequency_list = sorted(page_word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    #remove stop words if the user specified\n",
        "    if(search_mode):\n",
        "        sorted_word_frequency_list = remove_stop_words(sorted_word_frequency_list)\n",
        "\n",
        "    #sum the total words to calculate frequencies   \n",
        "    total_words_sum = 0\n",
        "    for key,value in sorted_word_frequency_list:\n",
        "        total_words_sum = total_words_sum + value\n",
        "\n",
        "    #just get the top 20 words\n",
        "    if len(sorted_word_frequency_list) > 20:\n",
        "        sorted_word_frequency_list = sorted_word_frequency_list[:20]\n",
        "\n",
        "    #create our final list which contains words, frequency (word count), percentage\n",
        "    final_list = []\n",
        "    for key,value in sorted_word_frequency_list:\n",
        "        percentage_value = float(value * 100) / total_words_sum\n",
        "        final_list.append([key, value, round(percentage_value, 4)])\n",
        "\n",
        "    #headers before the table\n",
        "    print_headers = ['Word', 'Frequency', 'Frequency Percentage']\n",
        "\n",
        "    #print the table with tabulate\n",
        "    print(tabulate(final_list, headers=print_headers, tablefmt='orgtbl'))\n",
        "\n",
        "#throw an exception in case it breaks\n",
        "except requests.exceptions.Timeout:\n",
        "    print(\"The server didn't respond. Please, try again later.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Word         |   Frequency |   Frequency Percentage |\n",
            "|--------------+-------------+------------------------|\n",
            "| actuarial    |          29 |                 2.5686 |\n",
            "| insurance    |          22 |                 1.9486 |\n",
            "| science      |          16 |                 1.4172 |\n",
            "| actuaries    |          14 |                 1.24   |\n",
            "| financial    |          14 |                 1.24   |\n",
            "| models       |          11 |                 0.9743 |\n",
            "| life         |          10 |                 0.8857 |\n",
            "| theory       |           9 |                 0.7972 |\n",
            "| methods      |           8 |                 0.7086 |\n",
            "| used         |           8 |                 0.7086 |\n",
            "| mathematical |           7 |                 0.62   |\n",
            "| many         |           7 |                 0.62   |\n",
            "| economics    |           7 |                 0.62   |\n",
            "| modern       |           7 |                 0.62   |\n",
            "| risk         |           6 |                 0.5314 |\n",
            "| s            |           6 |                 0.5314 |\n",
            "| future       |           6 |                 0.5314 |\n",
            "| pension      |           6 |                 0.5314 |\n",
            "| can          |           6 |                 0.5314 |\n",
            "| th           |           5 |                 0.4429 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}